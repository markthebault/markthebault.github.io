<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bigdata on Mark Thebault</title>
    <link>https://www.mark-thebault.pro/categories/bigdata/</link>
    <description>Recent content in Bigdata on Mark Thebault</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zn-Hans</language>
    <lastBuildDate>Sat, 30 Mar 2019 11:20:37 -0400</lastBuildDate>
    
	<atom:link href="https://www.mark-thebault.pro/categories/bigdata/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Apache Spark, the new way!</title>
      <link>https://www.mark-thebault.pro/articles/2019/spark-k8s/</link>
      <pubDate>Sat, 30 Mar 2019 11:20:37 -0400</pubDate>
      
      <guid>https://www.mark-thebault.pro/articles/2019/spark-k8s/</guid>
      <description>BigData analytics on Kubernetes Bigdata is a big topic since 2009, every company is seeking to process and extract information from the data that they stores. Making a complete environment from storing the data to presenting it in a simple way has been very challengin for the past years. Lot of solutions came out in the market filling just a small gap, but when you try to link everything together the BigData architecture become very complex and involves lot of different frameworks.
There is 4 different element of a bigdata environment: * Storing the data * Processing the data * Showing the analysis * Governing the data
In this short paper we will concentrate only on the second part, processing the data, more specifically processing batch data. There is lot of processing framework out there like Apache Spark, Apache Flink, Google DataFlow, Apache Pig&amp;hellip; The solution most conventionally used, it&amp;rsquo;s Apache Spark. I stated to use Spark since the version 1.5, today the last version is the 2.4.0. Lot of things have improved since back then. Before to run a spark cluster was not an easy task, either you have an Hadoop cluster and you run spark using Yarn as a scheduler.</description>
    </item>
    
    <item>
      <title>GDPR Compliant? Let&#39;s check!</title>
      <link>https://www.mark-thebault.pro/articles/2019/pii-data-checker/</link>
      <pubDate>Mon, 18 Feb 2019 20:14:59 +0800</pubDate>
      
      <guid>https://www.mark-thebault.pro/articles/2019/pii-data-checker/</guid>
      <description>Checking PII data on large datasets This repository contains a simple PySpark notebook that reads thought each line of a given Spark dataframe to extract all pii values. There is a check at two levels, the first one is at the column name level and the second one consists on checking each cell all the dataframe. Get started by cloning the repo:
git clone https://github.com/markthebault/pii-check-spark.git  Interpret the PII check If the check at the column level is positive, that does not necessarily means that the dataframe contains PII data. In case of a cell check level positive there is a very good chance that this dataframe contains pii data
The algorithm used Columns The Pii check consist in first checking the columns, typical columns names are used for pii value (such as name, address, phone&amp;hellip;.). The algorithms read the column name and perform a ratio of similarity to the tipical Pii data columns names. If the results is superior at 0.85 percent then we consider than the column is a PII column.
The cells To check if a cell contains a PII value, the algorithm runs it against some REGEX to check the value: - Name check - Email - Phone number - Street addresses - IPs - Credit card number In order to check the Names of the a person, in the python code the is a list of provided Names.</description>
    </item>
    
  </channel>
</rss>