<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mark Thebault</title>
    <link>https://www.mark-thebault.pro/</link>
    <description>Recent content on Mark Thebault</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zn-Hans</language>
    <lastBuildDate>Sat, 30 Mar 2019 11:20:37 -0400</lastBuildDate>
    
	<atom:link href="https://www.mark-thebault.pro/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Apache Spark, the new way!</title>
      <link>https://www.mark-thebault.pro/articles/2019/spark-k8s/</link>
      <pubDate>Sat, 30 Mar 2019 11:20:37 -0400</pubDate>
      
      <guid>https://www.mark-thebault.pro/articles/2019/spark-k8s/</guid>
      <description>BigData analytics on Kubernetes Bigdata is a big topic since 2009, every company is seeking to process and extract information from the data that they stores. Making a complete environment from storing the data to presenting it in a simple way has been very challengin for the past years. Lot of solutions came out in the market filling just a small gap, but when you try to link everything together the BigData architecture become very complex and involves lot of different frameworks.
There is 4 different element of a bigdata environment: * Storing the data * Processing the data * Showing the analysis * Governing the data
In this short paper we will concentrate only on the second part, processing the data, more specifically processing batch data. There is lot of processing framework out there like Apache Spark, Apache Flink, Google DataFlow, Apache Pig&amp;hellip; The solution most conventionally used, it&amp;rsquo;s Apache Spark. I stated to use Spark since the version 1.5, today the last version is the 2.4.0. Lot of things have improved since back then. Before to run a spark cluster was not an easy task, either you have an Hadoop cluster and you run spark using Yarn as a scheduler.</description>
    </item>
    
    <item>
      <title>Running Docker Swarm in AWS</title>
      <link>https://www.mark-thebault.pro/articles/2018/docker-swarm/</link>
      <pubDate>Fri, 01 Mar 2019 15:49:22 +0800</pubDate>
      
      <guid>https://www.mark-thebault.pro/articles/2018/docker-swarm/</guid>
      <description>Swarm cluster for AWS Setup a swarm cluster production ready haven&amp;rsquo;t been so easy !
This is a bootstrap project, which creates 6 swarm nodes, 3 managers and 3 workers.
All Swarm nodes are in a single private subnet and they have access to internet via a Nat Gateway in the public subnet.
There is also a Bastion Host to configure and SSH the swarm nodes. A default Elastic Load Balancer is created also that listens the trafic on TCP:3543 of all swarm nodes and load balance TCP trafic comming from the internet on the port 80.
You can find a more detailled view as above: Get started Clone the repo:
git clone https://github.com/markthebault/aws-swarm-cluster-for-production.git  Boostraping a Swarm cluster in AWS This project uses CoreOS images on alpha version (to get the last updates from docker)
1/ Provisionning of the infrastructure Make sure you environment contains the following variables:
export AWS_ACCESS_KEY_ID=&amp;lt;your access key&amp;gt; export AWS_SECRET_ACCESS_KEY=&amp;lt;your secrect key&amp;gt;  Create a file terraform.tfvars in ./terraform
Example:
control_cidr = &amp;quot;10.234.231.21/32&amp;quot; owner = &amp;quot;Mark&amp;quot; default_keypair_name = &amp;quot;swarm-clstr-kp&amp;quot; default_keypair_path = &amp;quot;~/.ssh/swarm-clstr-kp.pem&amp;quot;  Execute terraform plan to see what will be created and terraform apply to start terraforming ;)
Quick start To start even faster just run the make file make up, this will create the infrastructure and provisionning the VMs.</description>
    </item>
    
    <item>
      <title>GDPR Compliant? Let&#39;s check!</title>
      <link>https://www.mark-thebault.pro/articles/2019/pii-data-checker/</link>
      <pubDate>Mon, 18 Feb 2019 20:14:59 +0800</pubDate>
      
      <guid>https://www.mark-thebault.pro/articles/2019/pii-data-checker/</guid>
      <description>Checking PII data on large datasets This repository contains a simple PySpark notebook that reads thought each line of a given Spark dataframe to extract all pii values. There is a check at two levels, the first one is at the column name level and the second one consists on checking each cell all the dataframe. Get started by cloning the repo:
git clone https://github.com/markthebault/pii-check-spark.git  Interpret the PII check If the check at the column level is positive, that does not necessarily means that the dataframe contains PII data. In case of a cell check level positive there is a very good chance that this dataframe contains pii data
The algorithm used Columns The Pii check consist in first checking the columns, typical columns names are used for pii value (such as name, address, phone&amp;hellip;.). The algorithms read the column name and perform a ratio of similarity to the tipical Pii data columns names. If the results is superior at 0.85 percent then we consider than the column is a PII column.
The cells To check if a cell contains a PII value, the algorithm runs it against some REGEX to check the value: - Name check - Email - Phone number - Street addresses - IPs - Credit card number In order to check the Names of the a person, in the python code the is a list of provided Names.</description>
    </item>
    
    <item>
      <title>Build a Kubernetes cluster with Kops</title>
      <link>https://www.mark-thebault.pro/articles/2018/kops-kubernetes-aws/</link>
      <pubDate>Mon, 13 Aug 2018 00:14:19 +0800</pubDate>
      
      <guid>https://www.mark-thebault.pro/articles/2018/kops-kubernetes-aws/</guid>
      <description> Kubernetes cluster creation using Kops and terraform Create the kubernetes resources with terraform, generate the cluster configuration with Kops. This repo is an adaptation of this module
Get started Clone the repository and make it yours
git clone https://github.com/markthebault/kubernetes-kops-terraform.git  Create the cluster You need to have terraform and kop installed to your computer.
terraform init terraform plan --out=plan.out terraform apply plan.out cat ~/.kube/config  This cluster is private, that means there is no way to access it from the internet. You will need to use a bastion to access the cluster. A bastion host is provided with the key that is exported under key.pem. This key will not be committed, so don&amp;rsquo;t loose it ;)
Module usage the modules is located under ./modules/
module &amp;quot;kops&amp;quot; { source = &amp;quot;modules/tf-kops-cluster&amp;quot; sg_allow_ssh = &amp;quot;${aws_security_group.allow_ssh.id}&amp;quot; sg_allow_http_s = &amp;quot;${aws_security_group.allow_http.id}&amp;quot; cluster_name = &amp;quot;${var.cluster_name}&amp;quot; cluster_fqdn = &amp;quot;${var.cluster_name}.${aws_route53_zone.main.name}&amp;quot; route53_zone_id = &amp;quot;${aws_route53_zone.main.id}&amp;quot; kops_s3_bucket_arn = &amp;quot;${aws_s3_bucket.kops.arn}&amp;quot; kops_s3_bucket_id = &amp;quot;${aws_s3_bucket.kops.id}&amp;quot; vpc_id = &amp;quot;${module.vpc.vpc_id}&amp;quot; instance_key_name = &amp;quot;${aws_key_pair.generated_key.key_name}&amp;quot; internet_gateway_id = &amp;quot;${module.vpc.igw_id}&amp;quot; master_instance_type = &amp;quot;t2.medium&amp;quot; node_instance_type = &amp;quot;t2.medium&amp;quot; kubernetes_version = &amp;quot;${var.kubernetes_version}&amp;quot; kops_dns_mode = &amp;quot;private&amp;quot; public_subnet_cidr_blocks = &amp;quot;${var.kubernetes_public_subnets_cidr}&amp;quot; #For the LB private_subnet_ids = &amp;quot;${module.vpc.private_subnets}&amp;quot; }  </description>
    </item>
    
    <item>
      <title>Get connected! Transit VPC on AWS</title>
      <link>https://www.mark-thebault.pro/articles/2018/transit-vpc/</link>
      <pubDate>Wed, 01 Aug 2018 10:23:48 +0800</pubDate>
      
      <guid>https://www.mark-thebault.pro/articles/2018/transit-vpc/</guid>
      <description>Open Source Transit VPC on AWS This project will setup the necessary elements to create a transit VPC. I will include a packer step to build our router image. This router will be based on the VyOS 1.1.7. The AMI could also be found on the AWS Market place for a very decent price.
Terraform will be used to provision the infrastructure. Combinated with python scripts terraform will also generate the IPSec configuration. The IPSec configuration will be pushed by ssh, so make sure your private key is added to your ssh-agent.
Before starting You need to install terraform, packer and some python libs pip install -R tools/requirements.txt
Clone the repo:
git clone https://github.com/markthebault/aws-swarm-cluster-for-production.git  Run all You can easily deploy everything using the following command make all at the root project. This will lunch sequencialy the following makes: - vyos-image : create a vpc and build the AMI, tear down the vpc - terraform-network: create all the environment - vpn-generated-configurations: push the VPN conf into our VYOS router :)
VyOS generation In order to allow packer to build our AMI, packer needs to create an instance in a public subnet to boot and install the necessary dependencies for running VyOS.</description>
    </item>
    
  </channel>
</rss>